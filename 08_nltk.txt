Objective:  Tokenization of word and Sentences with the help of NLTK package

Theory: NLTK is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries, and an active discussion forum. 
Thanks to a hands-on guide introducing programming fundamentals alongside topics in computational linguistics, plus comprehensive API documentation, NLTK is suitable for linguists, engineers, students, educators, researchers, and industry users alike. NLTK is available for Windows, Mac OS X, and Linux. Best of all, NLTK is a free, open source, community-driven project. 
NLTK has been called “a wonderful tool for teaching, and working in, computational linguistics using Python,” and “an amazing library to play with natural language.”

Code:

import nltk
from nltk.tokenize import word_tokenize
 
nltk.download('punkt')
 
text = "Dr. APJ Abdul Kalam is a famous name in the whole world. \
He is counted among the greatest scientists of the 21st century.\
Even more, he becomes the 11th president of India and served his country.\
He was the most valued person of the country as his contribution as a scientist and as a president is beyond compare.\
Apart from that, his contribution to the ISRO (Indian Space Research Organization) is remarkable.\
He headed many projects that contributed to the society also he was the one who helped in the development of Agni and Prithvi missiles.\
For his involvement in the Nuclear power in India, he was known as Missile Man of India. And due to his contribution to the country, the government awarded him with the highest civilian award."

print('\n\nOutput of word tokenizer: \n', word_tokenize(text))
 
from nltk.tokenize import sent_tokenize
print("\n\nOutput of sentence tokenizer: \n", sent_tokenize(text))
 
from nltk.tokenize import WordPunctTokenizer
wp = WordPunctTokenizer()
print('\n\nOutput of Punctuation Tokenizer: \n', wp.tokenize(text))

Output:

https://lh6.googleusercontent.com/HCqkN9fuXjEpmi-tZQgMo4q1HnWGWD5HpEIEiWR7rAjm3bTAUH8ano2tX7KxHyVng-vexKwgu6edkUqCLOiLplx_sTCQ6RocIkLtjpwN3NY0SlYgc2Nq90MZi8qD6-VbIwOpc6OVMwcsDJX0Mg

https://lh5.googleusercontent.com/tTjcnRAvx8ZoeXJfhuh33afruy1w5BK819pP1iTgaa48_G_E1QEG-HgMw3imvzw0_wGZ5jShBS-OAMXl7BMm8l2h5801S4iJdCOfLl6uSgF7OCyPnlVtA5uhnNd7AiOVOhIVykxhRZkeKPLWMQ

https://lh3.googleusercontent.com/-4RIoAdTgCZ0Rdrxm46FfBrC8KLrSVHNq9AUU_jz-Z0ASJmSJoIJda105cTumZrsFJSdOPWG1GAXD0tHooGVdZJkLmeP7n2dp0GKJfAlPZEMc3j-5M4laG9W_fIGPye-3p67YHzjDMlurfDKAw